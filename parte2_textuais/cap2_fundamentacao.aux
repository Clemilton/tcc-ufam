\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{monard2003}
\citation{chappelle2006}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Fundamenta\IeC {\c c}\IeC {\~a}o Te\IeC {\'o}rica}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cap2}{{2}{4}{Fundamentação Teórica}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Aprendizado de M\IeC {\'a}quina}{4}{section.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Representa\IeC {\c c}\IeC {\~a}o da base de dados\relax }}{5}{table.caption.12}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table-dataset}{{2.1}{5}{Representação da base de dados\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Aprendizado Supervisionado}{5}{subsection.2.1.1}}
\citation{hart1967}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}K-Nearest Neighbors (KNN)}{6}{subsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Princ\IeC {\'\i }pio dos k-vizinhos mais pr\IeC {\'o}ximos\relax }}{6}{figure.caption.13}}
\newlabel{fig-knn}{{2.1}{6}{Princípio dos k-vizinhos mais próximos\relax }{figure.caption.13}{}}
\citation{zhang1998}
\citation{haykin2009}
\citation{ferneda2006}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Redes Neurais Artificiais}{7}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Inspira\IeC {\c c}\IeC {\~a}o Biol\IeC {\'o}gica e Perceptron}{7}{subsection.2.2.1}}
\citation{McCulloch1943}
\citation{lippman1987}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Representa\IeC {\c c}\IeC {\~a}o simplificada de um neur\IeC {\^o}nio\relax }}{8}{figure.caption.14}}
\newlabel{fig-neuronio}{{2.2}{8}{Representação simplificada de um neurônio\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Modelo matem\IeC {\'a}tico de um neur\IeC {\^o}nio\relax }}{8}{figure.caption.15}}
\newlabel{fig-perceptron}{{2.3}{8}{Modelo matemático de um neurônio\relax }{figure.caption.15}{}}
\newlabel{eq_activation}{{2.1}{8}{Inspiração Biológica e Perceptron}{equation.2.2.1}{}}
\newlabel{eq-output-percep}{{2.2}{8}{Inspiração Biológica e Perceptron}{equation.2.2.2}{}}
\citation{hinton1986}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Perceptron multicamadas}{9}{subsection.2.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Multilayer Perceptron. Cada c\IeC {\'\i }rculo representa um neur\IeC {\^o}nio mostrado anteriormente\relax }}{9}{figure.caption.16}}
\newlabel{fig-mlp}{{2.4}{9}{Multilayer Perceptron. Cada círculo representa um neurônio mostrado anteriormente\relax }{figure.caption.16}{}}
\citation{Goodfellow2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Camada Softmax}{10}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Fun\IeC {\c c}\IeC {\~a}o de Perda(Loss Function)}{10}{subsection.2.2.4}}
\newlabel{eq-eqm}{{2.4}{10}{Função de Perda(Loss Function)}{equation.2.2.4}{}}
\citation{bengio2012}
\citation{adagrad2011}
\citation{adam2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Otimizadores}{11}{subsection.2.2.5}}
\newlabel{ssec:otimizadores}{{2.2.5}{11}{Otimizadores}{subsection.2.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Hiperpar\IeC {\^a}metros de uma Rede Neural}{11}{subsection.2.2.6}}
\citation{universal1989}
\citation{towardsdatascience}
\citation{towardsdatascience}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6.1}N\IeC {\'u}mero de camadas e neur\IeC {\^o}nios em cada camada}{12}{subsubsection.2.2.6.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6.2}Inicializa\IeC {\c c}\IeC {\~a}o dos pesos}{12}{subsubsection.2.2.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6.3}Fun\IeC {\c c}\IeC {\~a}o de Ativa\IeC {\c c}\IeC {\~a}o}{12}{subsubsection.2.2.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Softmax vs ReLU. Imagem tirada do site \textit  {Towards Data Science} \cite  {towardsdatascience}\relax }}{12}{figure.caption.17}}
\newlabel{fig-activations}{{2.5}{12}{Softmax vs ReLU. Imagem tirada do site \textit {Towards Data Science} \cite {towardsdatascience}\relax }{figure.caption.17}{}}
\citation{dropout2014}
\citation{dropout2014}
\citation{dropout2014}
\citation{camporeceptivo}
\citation{camporeceptivo}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6.4}Mini-batches}{13}{subsubsection.2.2.6.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6.5}Taxa de aprendizado}{13}{subsubsection.2.2.6.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6.6}N\IeC {\'u}mero de \IeC {\'e}pocas}{13}{subsubsection.2.2.6.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Regulariza\IeC {\c c}\IeC {\~a}o}{13}{subsection.2.2.7}}
\citation{haykin2009neural}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Imagem ilustrando o efeito do Dropout em uma rede neural onde os neur\IeC {\^o}nios marcados foram desativados, for\IeC {\c c}ando a rede nao depender deles, evitando o \textit  {overfitting} \cite  {dropout2014}.\relax }}{14}{figure.caption.18}}
\newlabel{fig-dropout}{{2.6}{14}{Imagem ilustrando o efeito do Dropout em uma rede neural onde os neurônios marcados foram desativados, forçando a rede nao depender deles, evitando o \textit {overfitting} \cite {dropout2014}.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Redes Convolutivas}{14}{subsection.2.2.8}}
\newlabel{cnn}{{2.2.8}{14}{Redes Convolutivas}{subsection.2.2.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Campo receptivo de um neur\IeC {\^o}nio. Fonte: \cite  {camporeceptivo}\relax }}{14}{figure.caption.19}}
\newlabel{fig-campo}{{2.7}{14}{Campo receptivo de um neurônio. Fonte: \cite {camporeceptivo}\relax }{figure.caption.19}{}}
\citation{guedes2017}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.8.1}Camada de Convolu\IeC {\c c}\IeC {\~a}o}{15}{subsubsection.2.2.8.1}}
\citation{depth}
\citation{depth}
\citation{githubstride}
\citation{githubstride}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Disposi\IeC {\c c}\IeC {\~a}o dos neur\IeC {\^o}nios na camada de convolu\IeC {\c c}\IeC {\~a}o e seu campo receptivo. Fonte: \cite  {depth}\relax }}{16}{figure.caption.20}}
\newlabel{fig-depth}{{2.8}{16}{Disposição dos neurônios na camada de convolução e seu campo receptivo. Fonte: \cite {depth}\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Neur\IeC {\^o}nios espa\IeC {\c c}ados por stride 1 e 2 das esquerda para a direita respectivamente. Separados \IeC {\`a} direita est\IeC {\~a}o os pesos compartilhados entre todos os neur\IeC {\^o}nios. Fonte: \cite  {githubstride}\relax }}{16}{figure.caption.21}}
\newlabel{fig-stride}{{2.9}{16}{Neurônios espaçados por stride 1 e 2 das esquerda para a direita respectivamente. Separados à direita estão os pesos compartilhados entre todos os neurônios. Fonte: \cite {githubstride}\relax }{figure.caption.21}{}}
\citation{Goodfellow2016}
\citation{githubstride}
\citation{githubstride}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.8.2}Camada Pooling}{17}{subsubsection.2.2.8.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Representa\IeC {\c c}\IeC {\~a}o de max-pooling e average-pooling. Fonte: \cite  {githubstride}\relax }}{17}{figure.caption.22}}
\newlabel{fig-pooling}{{2.10}{17}{Representação de max-pooling e average-pooling. Fonte: \cite {githubstride}\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.8.3}Normaliza\IeC {\c c}\IeC {\~a}o em lote}{17}{subsubsection.2.2.8.3}}
\citation{lecun98}
\citation{lecun98}
\citation{resnet2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.8.4}Arquitetura de uma Rede Convolutiva}{18}{subsubsection.2.2.8.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Arquitetura LeNet-5. Fonte: \cite  {lecun98}\relax }}{18}{figure.caption.23}}
\newlabel{fig-alexnet}{{2.11}{18}{Arquitetura LeNet-5. Fonte: \cite {lecun98}\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}Redes Residuais}{18}{subsection.2.2.9}}
\citation{resnet2015}
\citation{resnet2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces Bloco Residual. Fonte: \cite  {resnet2015}\relax }}{19}{figure.caption.24}}
\newlabel{fig-resblock}{{2.12}{19}{Bloco Residual. Fonte: \cite {resnet2015}\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}S\IeC {\'e}ries Temporais}{19}{section.2.3}}
\newlabel{eq_TS}{{2.6}{19}{Séries Temporais}{equation.2.3.6}{}}
\citation{Maimon:2005:DMK:1088958}
\citation{UCRArchive}
\citation{Dau2018TheUT}
\citation{Maimon:2005:DMK:1088958}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Reposit\IeC {\'o}rio UCR}{20}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Aplica\IeC {\c c}\IeC {\~o}es de S\IeC {\'e}ries Temporais}{20}{subsection.2.3.2}}
\citation{ferraz2016}
\citation{ferraz2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Medidas de similaridade}{21}{subsection.2.3.3}}
\newlabel{diss}{{2.3.3}{21}{Medidas de similaridade}{subsection.2.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Dist\IeC {\^a}ncia Euclidiana. Fonte: \cite  {ferraz2016}\relax }}{21}{figure.caption.25}}
\newlabel{fig-distED}{{2.13}{21}{Distância Euclidiana. Fonte: \cite {ferraz2016}\relax }{figure.caption.25}{}}
\citation{Berndt:1994:UDT:3000850.3000887}
\citation{Keogh2005}
\citation{Keogh2005}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces A) Duas sequ\IeC {\^e}ncias Q e C s\IeC {\~a}o semelhantes, por\IeC {\'e}m fora de fase. B) Para alinhar as sequ\IeC {\^e}ncias, construirmos a matriz e procuramos pelo melhor caminho. C) O alinhamento resultado. Fonte: \cite  {Keogh2005}\relax }}{22}{figure.caption.26}}
\newlabel{fig-dtw}{{2.14}{22}{A) Duas sequências Q e C são semelhantes, porém fora de fase. B) Para alinhar as sequências, construirmos a matriz e procuramos pelo melhor caminho. C) O alinhamento resultado. Fonte: \cite {Keogh2005}\relax }{figure.caption.26}{}}
\citation{ferraz2016}
\citation{ferraz2016}
\citation{Deng:2013}
\citation{Keogh2003}
\citation{Wang2013}
\citation{Lines2015}
\citation{giusti2013}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces A similaridade entre as s\IeC {\'e}ries temporais A e B \IeC {\'e} reconhecida pela DTW. \cite  {ferraz2016}\relax }}{24}{figure.caption.27}}
\newlabel{fig-dtw2}{{2.15}{24}{A similaridade entre as séries temporais A e B é reconhecida pela DTW. \cite {ferraz2016}\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Classifica\IeC {\c c}\IeC {\~a}o de S\IeC {\'e}ries Temporais}{24}{section.2.4}}
\citation{ratanamahatana2004everything}
\citation{Ratanamahatana2004MakingTC}
\citation{Ratanamahatana2004MakingTC}
\citation{schafer2015}
\citation{bagnall2015}
\citation{ismail2018}
\citation{ismail2018}
\citation{ismail2018}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Visualiza\IeC {\c c}\IeC {\~a}o da janela de Sakoe-Chiba e do paralelogramo de Itakura. Fonte: \cite  {Ratanamahatana2004MakingTC}\relax }}{25}{figure.caption.28}}
\newlabel{fig-janeladtw}{{2.16}{25}{Visualização da janela de Sakoe-Chiba e do paralelogramo de Itakura. Fonte: \cite {Ratanamahatana2004MakingTC}\relax }{figure.caption.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Classifica\IeC {\c c}\IeC {\~a}o de series temporais com aprendizado profundo}{25}{subsection.2.4.1}}
\citation{autoencoder1}
\citation{autoencoder2}
\citation{autoencoder3}
\citation{autoencoder4}
\citation{ESN1}
\citation{ESN2}
\citation{marton2018}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces Vis\IeC {\~a}o geral de diferentes abordagens de aprendizado profundo para classifica\IeC {\c c}\IeC {\~a}o de s\IeC {\'e}ries temporais. Fonte: \cite  {ismail2018}\relax }}{26}{figure.caption.29}}
\newlabel{fig-dnn-overview}{{2.17}{26}{Visão geral de diferentes abordagens de aprendizado profundo para classificação de séries temporais. Fonte: \cite {ismail2018}\relax }{figure.caption.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.1}Modelos Generativos}{26}{subsubsection.2.4.1.1}}
\citation{mlmastery}
\citation{Wang01}
\citation{TRIPATHY2018}
\citation{Wang02}
\citation{Zheng2014}
\citation{tscFromScratch}
\citation{karim2018}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1.2}Modelos Discriminativos}{27}{subsubsection.2.4.1.2}}
\@setckpt{parte2_textuais/cap2_fundamentacao}{
\setcounter{page}{28}
\setcounter{equation}{9}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{4}
\setcounter{subsection}{1}
\setcounter{subsubsection}{2}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{17}
\setcounter{table}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{AM@survey}{0}
\setcounter{parentequation}{0}
\setcounter{float@type}{16}
\setcounter{NAT@ctr}{0}
\setcounter{LT@tables}{2}
\setcounter{LT@chunks}{1}
\setcounter{lstnumber}{1}
\setcounter{IEEEsubequation}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{35}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{algorithm}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{3}
}
