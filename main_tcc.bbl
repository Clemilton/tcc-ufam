\providecommand{\abntreprintinfo}[1]{%
 \citeonline{#1}}
\begin{thebibliography}{}
\providecommand{\abntrefinfo}[3]{}
\providecommand{\abntbstabout}[1]{}
\abntbstabout{v-1.9.2 }

\bibitem{towardsdatascience}
\abntrefinfo{{Towards Data Science}}{{Towards Data Science}}{2017}
{{Towards Data Science}. \emph{Activations Functions: Neural Networks}. 2017.
[Online; acessado em 18 de Dezembro, 2013].
Dispon{\'\i}vel em:
  $<$https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6$>$.}

\bibitem{dropout2014}
\abntrefinfo{Srivastava et al.}{SRIVASTAVA et al.}{2014}
{SRIVASTAVA, N. et al. Dropout: A simple way to prevent neural networks from
  overfitting.
\emph{Journal of Machine Learning Research}, v.~15, p. 1929--1958, 2014.
Dispon{\'\i}vel em: $<$http://jmlr.org/papers/v15/srivastava14a.html$>$.}

\bibitem{camporeceptivo}
\abntrefinfo{{neuroclusterbrain.com}}{{neuroclusterbrain.com}}{2017}
{{neuroclusterbrain.com}. Neuron model rf-psth.
2017.
[Online; acessado em 08 de Janeiro, 2019].
Dispon{\'\i}vel em: $<$http://neuroclusterbrain.com$>$.}

\bibitem{depth}
\abntrefinfo{APHEX34}{APHEX34}{2015}
{APHEX34. Input volume connected to a convolutional layer.
2015.
[Online; acessado em 08 de Janeiro, 2019].
Dispon{\'\i}vel em:
  $<$https://commons.wikimedia.org/wiki/File:Conv\underline{\hspace{.1in}}layer.png$>$.}

\bibitem{githubstride}
\abntrefinfo{KARPATHY}{KARPATHY}{2015}
{KARPATHY, A. \emph{Illustration of spatial arrangement}. 2015.
[Online; acessado em 17 de Janeiro, 2019].
Dispon{\'\i}vel em:
  $<$https://github.com/cs231n/cs231n.github.io/blob/master/assets/cnn/stride.jpeg$>$.}

\bibitem{lecun98}
\abntrefinfo{Lecun et al.}{LECUN et al.}{1998}
{LECUN, Y. et al. Gradient-based learning applied to document recognition. In:
  \emph{Proceedings of the IEEE}. [S.l.: s.n.], 1998. p. 2278--2324.}

\bibitem{resnet2015}
\abntrefinfo{He et al.}{HE et al.}{2015}
{HE, K. et al. Deep residual learning for image recognition.
\emph{CoRR}, abs/1512.03385, 2015.
Dispon{\'\i}vel em: $<$http://arxiv.org/abs/1512.03385$>$.}

\bibitem{ferraz2016}
\abntrefinfo{Amaral}{AMARAL}{2016}
{AMARAL, B.~F.

\emph{Classificação semissupervisionada de séries temporais extraídas de
  imagens de satélite} --- Instituto de Ciências Matemáticas e de
  Computação, Universidade de São Paulo, São Carlos, 2016. Dissertação de
  Mestrado em Ciências de Computação e Matemática Computacional.
  Disponível em:
  http://www.teses.usp.br/teses/disponiveis/55/55134/tde-18112016-105621/.}

\bibitem{Keogh2005}
\abntrefinfo{Keogh e Ratanamahatana}{KEOGH; RATANAMAHATANA}{2005}
{KEOGH, E.; RATANAMAHATANA, C.~A. Exact indexing of dynamic time warping.
\emph{Knowledge and Information Systems}, v.~7, n.~3, p. 358--386, Mar 2005.
ISSN 0219-3116.
Dispon{\'\i}vel em: $<$https://doi.org/10.1007/s10115-004-0154-9$>$.}

\bibitem{Ratanamahatana2004MakingTC}
\abntrefinfo{Ratanamahatana e Keogh}{RATANAMAHATANA; KEOGH}{2004}
{RATANAMAHATANA, C.; KEOGH, E.~J. Making time-series classification more
  accurate using learned constraints. In:  \emph{SDM}. [S.l.: s.n.], 2004.}

\bibitem{ismail2018}
\abntrefinfo{Fawaz et al.}{FAWAZ et al.}{2018}
{FAWAZ, H.~I. et al. Deep learning for time series classification: a review.
\emph{CoRR}, abs/1809.04356, 2018.
Dispon{\'\i}vel em: $<$http://arxiv.org/abs/1809.04356$>$.}

\bibitem{Bagnall2017}
\abntrefinfo{Bagnall et al.}{BAGNALL et al.}{2017}
{BAGNALL, A. et al. The great time series classification bake off: a review and
  experimental evaluation of recent algorithmic advances.
\emph{Data Mining and Knowledge Discovery}, v.~31, n.~3, p. 606--660, May 2017.
ISSN 1573-756X.
Dispon{\'\i}vel em: $<$https://doi.org/10.1007/s10618-016-0483-9$>$.}

\bibitem{monard2003}
\abntrefinfo{Monard e Baranauskas}{MONARD; BARANAUSKAS}{2003}
{MONARD, M.; BARANAUSKAS, J. \emph{Conceitos sobre aprendizado de máquinas. Em
  Sistemas Inteligentes: Fundamentos e Aplicações}. 1º. ed. [S.l.]: Editora
  Manole, 2003.}

\bibitem{chappelle2006}
\abntrefinfo{Chapelle, Schölkopf e Zien}{CHAPELLE; SCHöLKOPF; ZIEN}{2006}
{CHAPELLE, O.; SCHöLKOPF, B.; ZIEN, A. \emph{Semi-Supervised Learning.} 1º.
  ed. [S.l.]: MIT Press, Cambridge, 2006. 12 -- 13~p.}

\bibitem{hart1967}
\abntrefinfo{Cover e Hart}{COVER; HART}{1967}
{COVER, T.; HART, P. Nearest neighbor pattern classification.
\emph{Information Theory, IEEE Transaction on}, 1967.}

\bibitem{zhang1998}
\abntrefinfo{Zhang, Patuwo e Hu}{ZHANG; PATUWO; HU}{1998}
{ZHANG, G.; PATUWO, B.~E.; HU, M.~Y. Forecasting with artificial neural
  networks: The state of art.
\emph{International Journal of Forecasting}, 1998.}

\bibitem{haykin2009}
\abntrefinfo{Haykin}{HAYKIN}{}
{HAYKIN, S.~S. \emph{Neural networks and learning machines}. 3º. ed. Upper
  Saddle River, United States of America: Prentice Hall.}

\bibitem{ferneda2006}
\abntrefinfo{Ferneda}{FERNEDA}{2006}
{FERNEDA, E. Redes neurais e sua aplicação em sistemas de recuperação de
  informação.
\emph{Ciência da Informação}, v.~35, n.~1, 2006.
ISSN 1518-8353.
Dispon{\'\i}vel em: $<$http://revista.ibict.br/ciinf/article/view/1149$>$.}

\bibitem{McCulloch1943}
\abntrefinfo{McCulloch e Pitts}{MCCULLOCH; PITTS}{1943}
{MCCULLOCH, W.~S.; PITTS, W. A logical calculus of the ideas immanent in
  nervous activity.
\emph{The bulletin of mathematical biophysics}, 1943.
ISSN 1522-9602.
Dispon{\'\i}vel em: $<$https://doi.org/10.1007/BF02478259$>$.}

\bibitem{lippman1987}
\abntrefinfo{Lippmann}{LIPPMANN}{1987}
{LIPPMANN, R. An introduction to computing with neural nets.
\emph{IEEE ASSP Magazine}, v.~4, n.~2, 1987.}

\bibitem{hinton1986}
\abntrefinfo{Rumelhart, Hinton e Williams}{RUMELHART; HINTON; WILLIAMS}{1986}
{RUMELHART, D.~E.; HINTON, G.~E.; WILLIAMS, R.~J. Learning representations by
  back-propagating errors.
\emph{Nature}, Nature Publishing Group, v.~323, p. 533--, out. 1986.
Dispon{\'\i}vel em: $<$http://dx.doi.org/10.1038/323533a0$>$.}

\bibitem{Goodfellow2016}
\abntrefinfo{Goodfellow, Bengio e Courville}{GOODFELLOW; BENGIO;
  COURVILLE}{2016}
{GOODFELLOW, I.; BENGIO, Y.; COURVILLE, A. \emph{Deep Learning}. [S.l.]: MIT
  Press, 2016. \url{http://www.deeplearningbook.org}.}

\bibitem{bengio2012}
\abntrefinfo{Bengio, Boulanger{-}Lewandowski e Pascanu}{BENGIO;
  BOULANGER{-}LEWANDOWSKI; PASCANU}{2012}
{BENGIO, Y.; BOULANGER{-}LEWANDOWSKI, N.; PASCANU, R. Advances in optimizing
  recurrent networks.
\emph{CoRR}, abs/1212.0901, 2012.
Dispon{\'\i}vel em: $<$http://arxiv.org/abs/1212.0901$>$.}

\bibitem{adagrad2011}
\abntrefinfo{Duchi, Hazan e Singer}{DUCHI; HAZAN; SINGER}{2011}
{DUCHI, J. C.; HAZAN, E.; SINGER, Y. Adaptive subgradient methods for online
  learning and stochastic optimization.
\emph{Journal of Machine Learning Research}, v.~12, p. 2121--2159, 07 2011.}

\bibitem{adam2014}
\abntrefinfo{Kingma e Ba}{KINGMA; BA}{2014}
{KINGMA, D.~P.; BA, J. Adam: {A} method for stochastic optimization.
\emph{CoRR}, abs/1412.6980, 2014.
Dispon{\'\i}vel em: $<$http://arxiv.org/abs/1412.6980$>$.}

\bibitem{universal1989}
\abntrefinfo{Cybenko}{CYBENKO}{1989}
{CYBENKO, G. Approximation by superpositions of a sigmoidal function.
\emph{Mathematics of Control, Signals, and Systems. 1989 Springer-Verlag New
  York Inc}, p. 303--314, 1989.}

\bibitem{haykin2009neural}
\abntrefinfo{Haykin}{HAYKIN}{2009}
{HAYKIN, S.~S. \emph{Neural networks and learning machines}. Third. Upper
  Saddle River, NJ: Pearson Education, 2009.}

\bibitem{guedes2017}
\abntrefinfo{Guedes}{GUEDES}{2017}
{GUEDES, A.~B.
\emph{Reconhecimento de Gestos usando Redes Convolucionadas}.
 49~p. --- UnB - Universidade de Brasília, Brasília, 2017.}

\bibitem{Maimon:2005:DMK:1088958}
\abntrefinfo{Maimon e Rokach}{MAIMON; ROKACH}{2005}
{MAIMON, O.; ROKACH, L. \emph{Data Mining and Knowledge Discovery Handbook}.
  Berlin, Heidelberg: Springer-Verlag, 2005.
ISBN 0387244352, 9780387244358.}

\bibitem{UCRArchive}
\abntrefinfo{Chen et al.}{CHEN et al.}{2015}
{CHEN, Y. et al. \emph{The UCR Time Series Classification Archive}. 2015.
\url{www.cs.ucr.edu/~eamonn/time_series_data/}.}

\bibitem{Dau2018TheUT}
\abntrefinfo{Dau et al.}{DAU et al.}{2018}
{DAU, H.~A. et al. Ucr time series archive.
\emph{CoRR}, abs/1810.07758, 2018.}

\bibitem{Berndt:1994:UDT:3000850.3000887}
\abntrefinfo{Berndt e Clifford}{BERNDT; CLIFFORD}{1994}
{BERNDT, D.~J.; CLIFFORD, J. Using dynamic time warping to find patterns in
  time series. In:  \emph{Proceedings of the 3rd International Conference on
  Knowledge Discovery and Data Mining}. AAAI Press, 1994.  (AAAIWS'94), p.
  359--370. Dispon{\'\i}vel em:
  $<$http://dl.acm.org/citation.cfm?id=3000850.3000887$>$.}

\bibitem{Deng:2013}
\abntrefinfo{Deng et al.}{DENG et al.}{2013}
{DENG, H. et al. A time series forest for classification and feature
  extraction.
\emph{Inf. Sci.}, Elsevier Science Inc., New York, NY, USA, v.~239, p.
  142--153, ago. 2013.
ISSN 0020-0255.
Dispon{\'\i}vel em: $<$https://doi.org/10.1016/j.ins.2013.02.030$>$.}

\bibitem{Keogh2003}
\abntrefinfo{Keogh e Kasetty}{KEOGH; KASETTY}{2003}
{KEOGH, E.; KASETTY, S. On the need for time series data mining benchmarks: A
  survey and empirical demonstration.
\emph{Data Min. Knowl. Discov.}, Kluwer Academic Publishers, Hingham, MA, USA,
  v.~7, n.~4, p. 349--371, out. 2003.
ISSN 1384-5810.
Dispon{\'\i}vel em: $<$https://doi.org/10.1023/A:1024988512476$>$.}

\bibitem{Wang2013}
\abntrefinfo{Wang et al.}{WANG et al.}{2013}
{WANG, X. et al. Experimental comparison of representation methods and distance
  measures for time series data.
\emph{Data Mining and Knowledge Discovery}, v.~26, n.~2, p. 275--309, Mar 2013.
Dispon{\'\i}vel em: $<$https://doi.org/10.1007/s10618-012-0250-5$>$.}

\bibitem{Lines2015}
\abntrefinfo{Lines e Bagnall}{LINES; BAGNALL}{2015}
{LINES, J.; BAGNALL, A. Time series classification with ensembles of elastic
  distance measures.
\emph{Data Mining and Knowledge Discovery}, v.~29, n.~3, p. 565--592, May 2015.
Dispon{\'\i}vel em: $<$https://doi.org/10.1007/s10618-014-0361-2$>$.}

\bibitem{giusti2013}
\abntrefinfo{{Giusti} e {Batista}}{{Giusti}; {Batista}}{2013}
{{Giusti}, R.; {Batista}, G. E. A. P.~A. An empirical comparison of
  dissimilarity measures for time series classification. In:  \emph{2013
  Brazilian Conference on Intelligent Systems}. [S.l.: s.n.], 2013. p. 82--88.}

\bibitem{ratanamahatana2004everything}
\abntrefinfo{Ratanamahatana e Keogh}{RATANAMAHATANA; KEOGH}{2004}
{RATANAMAHATANA, C.~A.; KEOGH, E. Everything you know about dynamic time
  warping is wrong. In:  CITESEER. \emph{Third Workshop on Mining Temporal and
  Sequential Data}. [S.l.], 2004.}

\bibitem{schafer2015}
\abntrefinfo{Sch{\"a}fer}{SCH{\"A}FER}{2015}
{SCH{\"A}FER, P. The boss is concerned with time series classification in the
  presence of noise.
\emph{Data Mining and Knowledge Discovery}, v.~29, n.~6, p. 1505--1530, Nov
  2015.
ISSN 1573-756X.
Dispon{\'\i}vel em: $<$https://doi.org/10.1007/s10618-014-0377-7$>$.}

\bibitem{bagnall2015}
\abntrefinfo{Bagnall et al.}{BAGNALL et al.}{2015}
{BAGNALL, A. et al. Time-series classification with cote: The collective of
  transformation-based ensembles.
\emph{IEEE Transactions on Knowledge and Data Engineering}, v.~27, p.~1--1, 09
  2015.}

\bibitem{autoencoder1}
\abntrefinfo{Zhiguang et al.}{ZHIGUANG et al.}{2016}
{ZHIGUANG et al. Representation learning with deconvolution for multivariate
  time series classification and visualization.
\emph{CoRR}, abs/1610.07258, 2016.
Dispon{\'\i}vel em: $<$http://arxiv.org/abs/1610.07258$>$.}

\bibitem{autoencoder2}
\abntrefinfo{Mittelman}{MITTELMAN}{2015}
{MITTELMAN, R. Time-series modeling with undecimated fully convolutional neural
  networks.
08 2015.
Dispon{\'\i}vel em: $<$https://arxiv.org/abs/1508.00317$>$.}

\bibitem{autoencoder3}
\abntrefinfo{Mehdiyev et al.}{MEHDIYEV et al.}{2017}
{MEHDIYEV, N. et al. Time series classification using deep learning for process
  planning: A case from the process industry.
\emph{Procedia Computer Science}, v.~114, p. 242 -- 249, 2017.
ISSN 1877-0509.
Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical
  Systems, CAS October 30 – November 1, 2017, Chicago, Illinois, USA.
Dispon{\'\i}vel em:
  $<$http://www.sciencedirect.com/science/article/pii/S1877050917318707$>$.}

\bibitem{autoencoder4}
\abntrefinfo{Malhotra et al.}{MALHOTRA et al.}{2017}
{MALHOTRA, P. et al. Timenet: Pre-trained deep recurrent neural network for
  time series classification.
\emph{CoRR}, abs/1706.08838, 2017.
Dispon{\'\i}vel em: $<$http://arxiv.org/abs/1706.08838$>$.}

\bibitem{ESN1}
\abntrefinfo{Aswolinskiy, Reinhart e Steil}{ASWOLINSKIY; REINHART; STEIL}{2018}
{ASWOLINSKIY, W.; REINHART, R.~F.; STEIL, J. Time series classification in
  reservoir- and model-space.
\emph{Neural Processing Letters}, v.~48, n.~2, p. 789--809, Oct 2018.
ISSN 1573-773X.
Dispon{\'\i}vel em: $<$https://doi.org/10.1007/s11063-017-9765-5$>$.}

\bibitem{ESN2}
\abntrefinfo{Bianchi et al.}{BIANCHI et al.}{2018}
{BIANCHI, F.~M. et al. Reservoir computing approaches for representation and
  classification of multivariate time series.
03 2018.}

\bibitem{marton2018}
\abntrefinfo{Santos}{SANTOS}{2018}
{SANTOS, M.~S.

\emph{Classificadores neurais de treinamento rápido aplicados na
  identificação online de evento no detector de atlas} --- Universidade
  Federal da Bahia, Salvador, 2018. Dissertação de Mestrado em Engenharia
  Elétrica. Disponível em:
  http://www.ppgee.eng.ufba.br/teses/c2125147ac78f314a43ec5d461e07f73.pdf.}

\bibitem{mlmastery}
\abntrefinfo{{J. Browniee}}{{J. Browniee}}{2014}
{{J. Browniee}. \emph{Discover Feature Engineering, How to Engineer Features
  and How to Get Good at It}. 2014.
[Online; acessado em 11 de março, 2019].
Dispon{\'\i}vel em:
  $<$https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/$>$.}

\bibitem{Wang01}
\abntrefinfo{Wang e Oates}{WANG; OATES}{2015}
{WANG, Z.; OATES, T. Imaging time-series to improve classification and
  imputation. In:  \emph{Proceedings of the 24th International Conference on
  Artificial Intelligence}. AAAI Press, 2015.  (IJCAI'15), p. 3939--3945.
ISBN 978-1-57735-738-4. Dispon{\'\i}vel em:
  $<$http://dl.acm.org/citation.cfm?id=2832747.2832798$>$.}

\bibitem{TRIPATHY2018}
\abntrefinfo{Tripathy e Acharya}{TRIPATHY; ACHARYA}{2018}
{TRIPATHY, R.; ACHARYA, U.~R. Use of features from rr-time series and eeg
  signals for automated classification of sleep stages in deep neural network
  framework.
\emph{Biocybernetics and Biomedical Engineering}, v.~38, n.~4, p. 890 -- 902,
  2018.
ISSN 0208-5216.
Dispon{\'\i}vel em:
  $<$http://www.sciencedirect.com/science/article/pii/S0208521618300949$>$.}

\bibitem{Wang02}
\abntrefinfo{Wang e Oates}{WANG; OATES}{2015}
{WANG, Z.; OATES, T. Spatially encoding temporal correlations to classify
  temporal data using convolutional neural networks.
\emph{CoRR}, abs/1509.07481, 2015.
Dispon{\'\i}vel em: $<$http://arxiv.org/abs/1509.07481$>$.}

\bibitem{Zheng2014}
\abntrefinfo{Zheng et al.}{ZHENG et al.}{2014}
{ZHENG, Y. et al. Time series classification using multi-channels deep
  convolutional neural networks. In:  \emph{WAIM}. [S.l.: s.n.], 2014.}

\bibitem{tscFromScratch}
\abntrefinfo{Wang, Yan e Oates}{WANG; YAN; OATES}{2016}
{WANG, Z.; YAN, W.; OATES, T. Time series classification from scratch with deep
  neural networks: {A} strong baseline.
\emph{CoRR}, abs/1611.06455, 2016.
Dispon{\'\i}vel em: $<$http://arxiv.org/abs/1611.06455$>$.}

\bibitem{karim2018}
\abntrefinfo{{Karim} et al.}{{Karim} et al.}{2018}
{{Karim}, F. et al. Lstm fully convolutional networks for time series
  classification.
\emph{IEEE Access}, v.~6, p. 1662--1669, 2018.
ISSN 2169-3536.}

\end{thebibliography}
